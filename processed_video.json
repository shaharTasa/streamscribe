{
    "full_text": " In the coming years, artificial intelligence is probably going to change your life, and likely the entire world. But people have a hard time agreeing on exactly how. The following are excerpts from a world economic forum interview where renowned computer science professor and AI expert Stuart Russell help separate the sense from the nonsense. There's a big difference between asking a human to do something and giving that as the objective to an AI system. When you ask a human to fetch you a cup of coffee, you don't mean this should be their life's mission and nothing else in the universe matters. Even if they have to kill everybody else in Starbucks to get you the coffee before it closes, they should do that. No, that's not what you mean. You mean all the other things that we mutually care about, they should factor into your behavior as well. And the way we build AI systems now is we give them a fixed objective. The algorithms require us to specify everything in the objective. If you say, you know, we fix the acidification of the oceans, yeah, you could have a catalytic reaction that does that extremely efficiently, but it consumes the cordial of the oxygen in the atmosphere, which would apparently cause us to die fairly slowly and unpleasantly, because of course it's several hours. So how do we avoid this problem? You might say, okay, well, just be more careful about specifying the objective, right? Don't forget the atmospheric oxygen. And then of course, some side effect of the reaction in the ocean, poison's all the fish. Okay, well I meant don't kill the fish either. And then, well, what about the seaweed? Okay, don't do anything that's going to cause all the seaweed to die. And all in all in all, right? And the reason that we don't have to do that with humans is that humans often know that they don't know all the things that we care about. If you ask a human to get your cup coffee, you know, and you happen to be in the hotel of your sank in Paris where the coffee is, I think, 13 euros a cup. It's entirely reasonable to come back and say, well, it's 13 euros, are you sure you've won't or I could go next door and you'll get one? And it's a perfectly normal thing for a person to do, right? You ask, you know, I'm going to repaint your house. Is it okay if I take off the drain pipes and then put them back? We don't think of this as a terribly sophisticated capability, but AI systems don't have it, because the way we build them now, they have to know these four objectives. If we build systems that know that they don't know what the objective is, then they start to exhibit these behaviors, like asking permission before getting rid of all the obscenity out in the air. In all these sensors, control over the AI system comes from the machines uncertainty about what the true objective is. It's when you build machines that believe with certainty that they have the objectives. That's when you get the sort of psychopathic behavior, and I think you see the same thing in humans. What happens when General Purpose AI hits the real economy? How do things change? Can we adapt? This is a very old point. Amazingly, Aristotle actually has a passage where he says, look, if we had fully automated weaving machines and lecturers that could pluck a mire and use music without any humans, then we wouldn't need any work. That idea, which I think it was came to call it technological unemployment in 1930, is very obvious to people, right? They think, yeah, of course, if the machine does the work, then I'm going to be unemployed. If you think about the warehouses, the companies that currently operate for e-commerce, they are half automated. The way it works is that on old warehouses where you've got tons of stuff piled up all over the place, and humans go and rummage around and then bring it back and send it off. There's a robot who goes and gets the shelving unit that contains the thing that you need, but the human has to pick the object out of the bin or the off the shelf because that's still too difficult. But at the same time, would you make a robot that is accurate enough to be able to pick pretty much any object with almost a very wide variety of objects that you can buy? That would, at a stroke, eliminate three or four million jobs. There's an interesting story that EM Forster wrote where everyone is entirely machine dependent. The story is really about the fact that if you hand over the management of your civilization to machines, you then lose the incentive to understand it yourself or to teach the next generation how to understand it. And you can see, Wally, actually, as a modern version where everyone is in feeble and infantilized by machine, and that hasn't been possible up to now, right? We put a lot of our civilization into books, but the books can't run it for us, and so we always have to teach the next generation. If you work it out, it's about a trillion-person years of teaching and learning and an unbroken chain that goes back tens of thousands of generations. What happens if that chain breaks? And I think that's something we have to understand as AI needs forward. The actual date of arrival of General Purpose AI, you're not going to be able to pinpoint it, right? It isn't a single day. It's also not the case. It's all or nothing. The impact is going to be increasing. So with every advance in AI, it significantly expands the range of tasks. So in that sense, I think most experts say by the end of the century, we're very, very likely to have General Purpose AI. The medium is something around 2045. I'm a little more on the conservative side. I think the problem is harder than we think. I like what John McCarthy, he was one of the founders of AI. Well, he was asked this question. He said, well, somewhere between five and 500 years, and we're going to need, I think, several Einstein's to make it happen. But how will the economy change in the meantime? Will it be able to keep growing? Watch this video to hear economist Kate Rayworth explain why growth is not always a good thing, or continue to expand your understanding of economics on the World Economic Forum's YouTube channel.",
    "overall_summary": "Artificial intelligence is likely to change your life, and likely the entire world. Stuart Russell: When you ask a human to fetch you a cup of coffee, you don't mean this should be their life's mission and nothing else in the universe matters. When you build machines that believe with certainty that they have the objectives, that's when you get the sort of psychopathic behavior, Russell says.",
    "topics": [
        {
            "topic": "Topic 2",
            "text": " In the coming years, artificial intelligence is probably going to change your life, and likely the entire world. The following are excerpts from a world economic forum interview where renowned computer science professor and AI expert Stuart Russell help separate the sense from the nonsense. There's a big difference between asking a human to do something and giving that as the objective to an AI system. And the way we build AI systems now is we give them a fixed objective. The algorithms require us to specify everything in the objective. You might say, okay, well, just be more careful about specifying the objective, right? We don't think of this as a terribly sophisticated capability, but AI systems don't have it, because the way we build them now, they have to know these four objectives. If we build systems that know that they don't know what the objective is, then they start to exhibit these behaviors, like asking permission before getting rid of all the obscenity out in the air. In all these sensors, control over the AI system comes from the machines uncertainty about what the true objective is. It's when you build machines that believe with certainty that they have the objectives. What happens when General Purpose AI hits the real economy? Amazingly, Aristotle actually has a passage where he says, look, if we had fully automated weaving machines and lecturers that could pluck a mire and use music without any humans, then we wouldn't need any work. That idea, which I think it was came to call it technological unemployment in 1930, is very obvious to people, right? If you think about the warehouses, the companies that currently operate for e-commerce, they are half automated. But at the same time, would you make a robot that is accurate enough to be able to pick pretty much any object with almost a very wide variety of objects that you can buy? There's an interesting story that EM Forster wrote where everyone is entirely machine dependent. And you can see, Wally, actually, as a modern version where everyone is in feeble and infantilized by machine, and that hasn't been possible up to now, right? And I think that's something we have to understand as AI needs forward. The actual date of arrival of General Purpose AI, you're not going to be able to pinpoint it, right? So with every advance in AI, it significantly expands the range of tasks. So in that sense, I think most experts say by the end of the century, we're very, very likely to have General Purpose AI. I like what John McCarthy, he was one of the founders of AI."
        },
        {
            "topic": "Topic 5",
            "text": "But people have a hard time agreeing on exactly how. No, that's not what you mean. So how do we avoid this problem? Don't forget the atmospheric oxygen. Okay, well I meant don't kill the fish either. And all in all in all, right? And it's a perfectly normal thing for a person to do, right? How do things change? This is a very old point. It's also not the case. It's all or nothing. I'm a little more on the conservative side. I think the problem is harder than we think. Well, he was asked this question."
        },
        {
            "topic": "Topic 3",
            "text": "When you ask a human to fetch you a cup of coffee, you don't mean this should be their life's mission and nothing else in the universe matters. Even if they have to kill everybody else in Starbucks to get you the coffee before it closes, they should do that. You mean all the other things that we mutually care about, they should factor into your behavior as well. If you say, you know, we fix the acidification of the oceans, yeah, you could have a catalytic reaction that does that extremely efficiently, but it consumes the cordial of the oxygen in the atmosphere, which would apparently cause us to die fairly slowly and unpleasantly, because of course it's several hours. And then of course, some side effect of the reaction in the ocean, poison's all the fish. And then, well, what about the seaweed? Okay, don't do anything that's going to cause all the seaweed to die. And the reason that we don't have to do that with humans is that humans often know that they don't know all the things that we care about. If you ask a human to get your cup coffee, you know, and you happen to be in the hotel of your sank in Paris where the coffee is, I think, 13 euros a cup. It's entirely reasonable to come back and say, well, it's 13 euros, are you sure you've won't or I could go next door and you'll get one? That's when you get the sort of psychopathic behavior, and I think you see the same thing in humans. Can we adapt? They think, yeah, of course, if the machine does the work, then I'm going to be unemployed. The way it works is that on old warehouses where you've got tons of stuff piled up all over the place, and humans go and rummage around and then bring it back and send it off. There's a robot who goes and gets the shelving unit that contains the thing that you need, but the human has to pick the object out of the bin or the off the shelf because that's still too difficult."
        },
        {
            "topic": "Topic 1",
            "text": "You ask, you know, I'm going to repaint your house. Is it okay if I take off the drain pipes and then put them back?"
        },
        {
            "topic": "Topic 4",
            "text": "That would, at a stroke, eliminate three or four million jobs. The story is really about the fact that if you hand over the management of your civilization to machines, you then lose the incentive to understand it yourself or to teach the next generation how to understand it. We put a lot of our civilization into books, but the books can't run it for us, and so we always have to teach the next generation. If you work it out, it's about a trillion-person years of teaching and learning and an unbroken chain that goes back tens of thousands of generations. What happens if that chain breaks? It isn't a single day. The impact is going to be increasing. The medium is something around 2045. He said, well, somewhere between five and 500 years, and we're going to need, I think, several Einstein's to make it happen. But how will the economy change in the meantime? Will it be able to keep growing? Watch this video to hear economist Kate Rayworth explain why growth is not always a good thing, or continue to expand your understanding of economics on the World Economic Forum's YouTube channel."
        }
    ]
}